{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14666713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|████████████████████████████████████████████████████████████████▋                 | 15/19 [07:14<01:54, 28.56s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the path to the directory\n",
    "directory_path = r\"D:\\datasets\\Underwater_Image\\WHOI\\archive\\dataset_pm\\training\"\n",
    "\n",
    "# Initialize empty lists for images and labels\n",
    "X_train = []\n",
    "Y_train = []\n",
    "class_label = 0\n",
    "\n",
    "# Loop through subdirectories (classes)\n",
    "for class_folder in tqdm(sorted(os.listdir(directory_path))):\n",
    "    class_path = os.path.join(directory_path, class_folder)\n",
    "    \n",
    "    for image_file in os.listdir(class_path):\n",
    "        if image_file.endswith('.png'):\n",
    "            image_path = os.path.join(class_path, image_file)\n",
    "            \n",
    "            # Load image, convert to RGB and resize\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            img = img.resize((224, 224))\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # Append image and label to lists\n",
    "            X_train.append(img_array)\n",
    "            Y_train.append(class_label)\n",
    "            \n",
    "    class_label = class_label + 1\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the path to the directory\n",
    "directory_path = r\"D:\\datasets\\Underwater_Image\\WHOI\\archive\\dataset_pm\\validation\"\n",
    "\n",
    "# Initialize empty lists for images and labels\n",
    "X_val = []\n",
    "Y_val = []\n",
    "class_label = 0\n",
    "\n",
    "# Loop through subdirectories (classes)\n",
    "for class_folder in tqdm(sorted(os.listdir(directory_path))):\n",
    "    class_path = os.path.join(directory_path, class_folder)\n",
    "    \n",
    "    for image_file in os.listdir(class_path):\n",
    "        if image_file.endswith('.png'):\n",
    "            image_path = os.path.join(class_path, image_file)\n",
    "            \n",
    "            # Load image, convert to RGB and resize\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            img = img.resize((224, 224))\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # Append image and label to lists\n",
    "            X_val.append(img_array)\n",
    "            Y_val.append(class_label)\n",
    "            \n",
    "    class_label = class_label + 1\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_val = np.array(X_val)\n",
    "Y_val = np.array(Y_val)\n",
    "\n",
    "print(f'X_train shape: {X_val.shape}')\n",
    "print(f'Y_train shape: {Y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the path to the directory\n",
    "directory_path = r\"D:\\datasets\\Underwater_Image\\WHOI\\archive\\dataset_pm\\test\"\n",
    "\n",
    "# Initialize empty lists for images and labels\n",
    "X_test = []\n",
    "Y_test = []\n",
    "class_label = 0\n",
    "\n",
    "# Loop through subdirectories (classes)\n",
    "for class_folder in tqdm(sorted(os.listdir(directory_path))):\n",
    "    class_path = os.path.join(directory_path, class_folder)\n",
    "    \n",
    "    for image_file in os.listdir(class_path):\n",
    "        if image_file.endswith('.png'):\n",
    "            image_path = os.path.join(class_path, image_file)\n",
    "            \n",
    "            # Load image, convert to RGB and resize\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            img = img.resize((224, 224))\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # Append image and label to lists\n",
    "            X_test.append(img_array)\n",
    "            Y_test.append(class_label)\n",
    "            \n",
    "    class_label = class_label + 1\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print(f'X_train shape: {X_test.shape}')\n",
    "\n",
    "print(f'Y_train shape: {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c23d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "tf.keras.backend.clear_session()\n",
    "YT = to_categorical(Y_train)\n",
    "VT = to_categorical(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.resnet import ResNet50, preprocess_input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "\n",
    "num_classes = 19\n",
    "# Create the ResNet model without the top (fully connected) layers\n",
    "base_model = MobileNet(weights=None, include_top=False, input_shape=(224, 224, 3))    \n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Change units to match the number of classes\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "epochs = 50\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "history = model.fit(X_train, YT, epochs=epochs, batch_size=8, validation_data=(X_val, VT), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2facd632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convert one-hot encoded predictions back to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, predicted_labels)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(Y_test, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1def5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "359/359 [==============================] - 28s 51ms/step - loss: 2.0036 - accuracy: 0.3907 - val_loss: 4.6335 - val_accuracy: 0.2581\n",
      "Epoch 2/50\n",
      "359/359 [==============================] - 17s 48ms/step - loss: 1.2338 - accuracy: 0.6166 - val_loss: 4.1413 - val_accuracy: 0.3871\n",
      "Epoch 3/50\n",
      "359/359 [==============================] - 17s 47ms/step - loss: 0.8248 - accuracy: 0.7413 - val_loss: 3.1214 - val_accuracy: 0.3871\n",
      "Epoch 4/50\n",
      "359/359 [==============================] - 16s 46ms/step - loss: 0.6555 - accuracy: 0.8040 - val_loss: 2.1879 - val_accuracy: 0.4839\n",
      "Epoch 5/50\n",
      "359/359 [==============================] - 16s 45ms/step - loss: 0.5018 - accuracy: 0.8499 - val_loss: 1.9223 - val_accuracy: 0.5591\n",
      "Epoch 6/50\n",
      "359/359 [==============================] - 17s 48ms/step - loss: 0.4113 - accuracy: 0.8722 - val_loss: 2.6745 - val_accuracy: 0.3763\n",
      "Epoch 7/50\n",
      "359/359 [==============================] - 18s 51ms/step - loss: 0.3647 - accuracy: 0.8858 - val_loss: 3.2346 - val_accuracy: 0.4194\n",
      "Epoch 8/50\n",
      "359/359 [==============================] - 20s 55ms/step - loss: 0.2657 - accuracy: 0.9140 - val_loss: 2.2250 - val_accuracy: 0.5161\n",
      "Epoch 9/50\n",
      "359/359 [==============================] - 18s 50ms/step - loss: 0.2494 - accuracy: 0.9196 - val_loss: 1.8580 - val_accuracy: 0.6344\n",
      "Epoch 10/50\n",
      "359/359 [==============================] - 18s 51ms/step - loss: 0.1920 - accuracy: 0.9419 - val_loss: 2.8049 - val_accuracy: 0.4839\n",
      "Epoch 11/50\n",
      "359/359 [==============================] - 19s 52ms/step - loss: 0.1982 - accuracy: 0.9384 - val_loss: 4.0646 - val_accuracy: 0.1613\n",
      "Epoch 12/50\n",
      "359/359 [==============================] - 19s 52ms/step - loss: 0.1849 - accuracy: 0.9387 - val_loss: 1.8008 - val_accuracy: 0.6452\n",
      "Epoch 13/50\n",
      "359/359 [==============================] - 19s 52ms/step - loss: 0.1405 - accuracy: 0.9589 - val_loss: 1.6116 - val_accuracy: 0.6667\n",
      "Epoch 14/50\n",
      "359/359 [==============================] - 20s 56ms/step - loss: 0.1113 - accuracy: 0.9655 - val_loss: 1.8813 - val_accuracy: 0.6667\n",
      "Epoch 15/50\n",
      "359/359 [==============================] - 20s 56ms/step - loss: 0.1466 - accuracy: 0.9519 - val_loss: 3.3224 - val_accuracy: 0.5054\n",
      "Epoch 16/50\n",
      "359/359 [==============================] - 19s 54ms/step - loss: 0.1028 - accuracy: 0.9669 - val_loss: 2.2249 - val_accuracy: 0.6237\n",
      "Epoch 17/50\n",
      "359/359 [==============================] - 19s 53ms/step - loss: 0.1402 - accuracy: 0.9589 - val_loss: 2.3667 - val_accuracy: 0.6237\n",
      "Epoch 18/50\n",
      "359/359 [==============================] - 20s 55ms/step - loss: 0.1188 - accuracy: 0.9638 - val_loss: 3.1457 - val_accuracy: 0.4624\n",
      "Epoch 19/50\n",
      "359/359 [==============================] - 20s 57ms/step - loss: 0.1159 - accuracy: 0.9652 - val_loss: 1.7326 - val_accuracy: 0.6559\n",
      "Epoch 20/50\n",
      "359/359 [==============================] - 21s 59ms/step - loss: 0.1072 - accuracy: 0.9673 - val_loss: 1.4862 - val_accuracy: 0.7312\n",
      "Epoch 21/50\n",
      "359/359 [==============================] - 20s 57ms/step - loss: 0.0562 - accuracy: 0.9836 - val_loss: 1.5772 - val_accuracy: 0.6667\n",
      "Epoch 22/50\n",
      "359/359 [==============================] - 20s 56ms/step - loss: 0.0620 - accuracy: 0.9808 - val_loss: 1.7657 - val_accuracy: 0.6989\n",
      "Epoch 23/50\n",
      "359/359 [==============================] - 20s 55ms/step - loss: 0.1251 - accuracy: 0.9617 - val_loss: 1.7774 - val_accuracy: 0.6559\n",
      "Epoch 24/50\n",
      "359/359 [==============================] - 20s 56ms/step - loss: 0.0562 - accuracy: 0.9826 - val_loss: 1.7878 - val_accuracy: 0.6774\n",
      "Epoch 25/50\n",
      "359/359 [==============================] - 20s 57ms/step - loss: 0.0911 - accuracy: 0.9746 - val_loss: 2.4133 - val_accuracy: 0.4731\n",
      "Epoch 26/50\n",
      "359/359 [==============================] - 21s 58ms/step - loss: 0.0729 - accuracy: 0.9760 - val_loss: 2.0585 - val_accuracy: 0.5591\n",
      "Epoch 27/50\n",
      "359/359 [==============================] - 22s 61ms/step - loss: 0.1330 - accuracy: 0.9593 - val_loss: 3.4636 - val_accuracy: 0.5161\n",
      "Epoch 28/50\n",
      "359/359 [==============================] - 21s 58ms/step - loss: 0.0493 - accuracy: 0.9861 - val_loss: 2.0978 - val_accuracy: 0.6237\n",
      "Epoch 29/50\n",
      "359/359 [==============================] - 19s 54ms/step - loss: 0.0491 - accuracy: 0.9857 - val_loss: 2.0644 - val_accuracy: 0.6989\n",
      "Epoch 30/50\n",
      "359/359 [==============================] - 21s 58ms/step - loss: 0.0374 - accuracy: 0.9889 - val_loss: 1.8096 - val_accuracy: 0.6882\n",
      "Epoch 31/50\n",
      "359/359 [==============================] - 22s 61ms/step - loss: 0.0568 - accuracy: 0.9826 - val_loss: 3.1851 - val_accuracy: 0.6237\n",
      "Epoch 32/50\n",
      "359/359 [==============================] - 23s 63ms/step - loss: 0.0761 - accuracy: 0.9777 - val_loss: 2.3801 - val_accuracy: 0.5699\n",
      "Epoch 33/50\n",
      "359/359 [==============================] - 21s 58ms/step - loss: 0.0392 - accuracy: 0.9878 - val_loss: 1.5475 - val_accuracy: 0.7097\n",
      "Epoch 34/50\n",
      "359/359 [==============================] - 21s 58ms/step - loss: 0.0196 - accuracy: 0.9944 - val_loss: 1.8192 - val_accuracy: 0.7634\n",
      "Epoch 35/50\n",
      "359/359 [==============================] - 21s 59ms/step - loss: 0.0885 - accuracy: 0.9753 - val_loss: 2.2665 - val_accuracy: 0.6774\n",
      "Epoch 36/50\n",
      "359/359 [==============================] - 21s 59ms/step - loss: 0.0602 - accuracy: 0.9788 - val_loss: 2.4541 - val_accuracy: 0.6667\n",
      "Epoch 37/50\n",
      "359/359 [==============================] - 21s 59ms/step - loss: 0.0665 - accuracy: 0.9819 - val_loss: 2.3772 - val_accuracy: 0.6237\n",
      "Epoch 38/50\n",
      "359/359 [==============================] - 21s 58ms/step - loss: 0.0578 - accuracy: 0.9798 - val_loss: 1.9414 - val_accuracy: 0.6882\n",
      "Epoch 39/50\n",
      "359/359 [==============================] - 19s 53ms/step - loss: 0.0346 - accuracy: 0.9920 - val_loss: 2.3247 - val_accuracy: 0.6559\n",
      "Epoch 40/50\n",
      "359/359 [==============================] - 22s 62ms/step - loss: 0.0520 - accuracy: 0.9822 - val_loss: 2.0304 - val_accuracy: 0.6774\n",
      "Epoch 41/50\n",
      "359/359 [==============================] - 23s 63ms/step - loss: 0.0528 - accuracy: 0.9833 - val_loss: 2.0149 - val_accuracy: 0.6989\n",
      "Epoch 42/50\n",
      "359/359 [==============================] - 23s 65ms/step - loss: 0.0329 - accuracy: 0.9903 - val_loss: 2.4998 - val_accuracy: 0.5484\n",
      "Epoch 43/50\n",
      "359/359 [==============================] - 25s 69ms/step - loss: 0.0301 - accuracy: 0.9892 - val_loss: 2.2524 - val_accuracy: 0.6559\n",
      "Epoch 44/50\n",
      "359/359 [==============================] - 25s 70ms/step - loss: 0.0760 - accuracy: 0.9760 - val_loss: 2.3935 - val_accuracy: 0.6882\n",
      "Epoch 45/50\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 0.0611 - accuracy: 0.9819 - val_loss: 1.7894 - val_accuracy: 0.6989\n",
      "Epoch 46/50\n",
      "359/359 [==============================] - 23s 64ms/step - loss: 0.0363 - accuracy: 0.9889 - val_loss: 1.9773 - val_accuracy: 0.7204\n",
      "Epoch 47/50\n",
      "359/359 [==============================] - 24s 66ms/step - loss: 0.0473 - accuracy: 0.9850 - val_loss: 2.1984 - val_accuracy: 0.6344\n",
      "Epoch 48/50\n",
      "359/359 [==============================] - 22s 62ms/step - loss: 0.0316 - accuracy: 0.9892 - val_loss: 1.2961 - val_accuracy: 0.7849\n",
      "Epoch 49/50\n",
      "359/359 [==============================] - 21s 58ms/step - loss: 0.0266 - accuracy: 0.9913 - val_loss: 1.9687 - val_accuracy: 0.6774\n",
      "Epoch 50/50\n",
      "359/359 [==============================] - 20s 55ms/step - loss: 0.0093 - accuracy: 0.9979 - val_loss: 1.6839 - val_accuracy: 0.6882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e78d1dc760>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.resnet import ResNet50, preprocess_input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "import keras\n",
    "\n",
    "num_classes = 19\n",
    "model = keras.models.load_model(r\"F:\\Pre-Trained_Models\\Pre-Trained_Models\\mob_whoi_224_model.h5\")\n",
    "\n",
    "x = model.layers[-4].output  # Access the last 4th layer from the end\n",
    "x = GlobalAveragePooling2D()(x)  # Explicitly specify keepdims\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "new_model = keras.models.Model(inputs=model.input, outputs=output) \n",
    "new_model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "epochs = 50\n",
    "new_model.fit(X_train,YT, epochs=epochs,batch_size=8, validation_data=(X_val,VT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3add546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db00aa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9915254237288136\n",
      "F1 Score: 0.9915254237288136\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = new_model.predict(X_test)\n",
    "\n",
    "# Convert one-hot encoded predictions back to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, predicted_labels)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(Y_test, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf7968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "394bdf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "359/359 [==============================] - 508s 1s/step - loss: 3.0394 - accuracy: 0.1835 - val_loss: 2.4462 - val_accuracy: 0.1075\n",
      "Epoch 2/50\n",
      "359/359 [==============================] - 551s 2s/step - loss: 2.3183 - accuracy: 0.2653 - val_loss: 1.8801 - val_accuracy: 0.2903\n",
      "Epoch 3/50\n",
      "359/359 [==============================] - 602s 2s/step - loss: 1.9772 - accuracy: 0.3524 - val_loss: 2.0454 - val_accuracy: 0.3226\n",
      "Epoch 4/50\n",
      "359/359 [==============================] - 678s 2s/step - loss: 1.6606 - accuracy: 0.4614 - val_loss: 1.8060 - val_accuracy: 0.3333\n",
      "Epoch 5/50\n",
      "359/359 [==============================] - 409s 1s/step - loss: 1.3273 - accuracy: 0.5790 - val_loss: 1.6974 - val_accuracy: 0.4409\n",
      "Epoch 6/50\n",
      "359/359 [==============================] - 401s 1s/step - loss: 1.0553 - accuracy: 0.6793 - val_loss: 1.5935 - val_accuracy: 0.4624\n",
      "Epoch 7/50\n",
      "359/359 [==============================] - 406s 1s/step - loss: 0.9091 - accuracy: 0.7159 - val_loss: 1.4694 - val_accuracy: 0.5591\n",
      "Epoch 8/50\n",
      "359/359 [==============================] - 405s 1s/step - loss: 0.7293 - accuracy: 0.7845 - val_loss: 1.7432 - val_accuracy: 0.5484\n",
      "Epoch 9/50\n",
      "359/359 [==============================] - 424s 1s/step - loss: 0.5903 - accuracy: 0.8304 - val_loss: 1.5179 - val_accuracy: 0.5699\n",
      "Epoch 10/50\n",
      "359/359 [==============================] - 416s 1s/step - loss: 0.4816 - accuracy: 0.8541 - val_loss: 1.8269 - val_accuracy: 0.5269\n",
      "Epoch 11/50\n",
      "359/359 [==============================] - 419s 1s/step - loss: 0.3892 - accuracy: 0.8837 - val_loss: 1.7230 - val_accuracy: 0.5806\n",
      "Epoch 12/50\n",
      "359/359 [==============================] - 410s 1s/step - loss: 0.3894 - accuracy: 0.8823 - val_loss: 1.4754 - val_accuracy: 0.6452\n",
      "Epoch 13/50\n",
      "359/359 [==============================] - 403s 1s/step - loss: 0.3375 - accuracy: 0.8976 - val_loss: 1.6677 - val_accuracy: 0.5806\n",
      "Epoch 14/50\n",
      "359/359 [==============================] - 404s 1s/step - loss: 0.2706 - accuracy: 0.9154 - val_loss: 1.4309 - val_accuracy: 0.6344\n",
      "Epoch 15/50\n",
      "359/359 [==============================] - 410s 1s/step - loss: 0.3150 - accuracy: 0.9136 - val_loss: 2.3959 - val_accuracy: 0.4624\n",
      "Epoch 16/50\n",
      "359/359 [==============================] - 420s 1s/step - loss: 0.4732 - accuracy: 0.8621 - val_loss: 1.5399 - val_accuracy: 0.6237\n",
      "Epoch 17/50\n",
      "359/359 [==============================] - 416s 1s/step - loss: 0.1429 - accuracy: 0.9558 - val_loss: 1.6025 - val_accuracy: 0.6344\n",
      "Epoch 18/50\n",
      "359/359 [==============================] - 421s 1s/step - loss: 0.2366 - accuracy: 0.9318 - val_loss: 1.3326 - val_accuracy: 0.6344\n",
      "Epoch 19/50\n",
      "359/359 [==============================] - 408s 1s/step - loss: 0.1545 - accuracy: 0.9499 - val_loss: 1.2285 - val_accuracy: 0.6989\n",
      "Epoch 20/50\n",
      "359/359 [==============================] - 406s 1s/step - loss: 0.1209 - accuracy: 0.9617 - val_loss: 1.7724 - val_accuracy: 0.6022\n",
      "Epoch 21/50\n",
      "359/359 [==============================] - 409s 1s/step - loss: 0.2094 - accuracy: 0.9377 - val_loss: 2.1327 - val_accuracy: 0.6022\n",
      "Epoch 22/50\n",
      "359/359 [==============================] - 414s 1s/step - loss: 0.1951 - accuracy: 0.9419 - val_loss: 2.0103 - val_accuracy: 0.6237\n",
      "Epoch 23/50\n",
      "359/359 [==============================] - 417s 1s/step - loss: 0.0667 - accuracy: 0.9805 - val_loss: 1.6427 - val_accuracy: 0.6774\n",
      "Epoch 24/50\n",
      "359/359 [==============================] - 414s 1s/step - loss: 0.0806 - accuracy: 0.9774 - val_loss: 1.8399 - val_accuracy: 0.6989\n",
      "Epoch 25/50\n",
      "359/359 [==============================] - 413s 1s/step - loss: 0.2386 - accuracy: 0.9300 - val_loss: 1.8333 - val_accuracy: 0.6452\n",
      "Epoch 26/50\n",
      "359/359 [==============================] - 414s 1s/step - loss: 0.0981 - accuracy: 0.9694 - val_loss: 1.8480 - val_accuracy: 0.6667\n",
      "Epoch 27/50\n",
      "359/359 [==============================] - 403s 1s/step - loss: 0.0529 - accuracy: 0.9826 - val_loss: 2.0088 - val_accuracy: 0.6667\n",
      "Epoch 28/50\n",
      "359/359 [==============================] - 401s 1s/step - loss: 0.0719 - accuracy: 0.9774 - val_loss: 2.1692 - val_accuracy: 0.6452\n",
      "Epoch 29/50\n",
      "359/359 [==============================] - 417s 1s/step - loss: 0.1415 - accuracy: 0.9533 - val_loss: 1.7851 - val_accuracy: 0.6667\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00029: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.resnet import ResNet50, preprocess_input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend\n",
    "from keras import layers\n",
    "from keras import utils\n",
    "import keras\n",
    "import tensorflow\n",
    "\n",
    "class LayerScale(layers.Layer):\n",
    "    \"\"\"Layer scale module.\n",
    "\n",
    "    References:\n",
    "      - https://arxiv.org/abs/2103.17239\n",
    "\n",
    "    Args:\n",
    "      init_values (float): Initial value for layer scale. Should be within\n",
    "        [0, 1].\n",
    "      projection_dim (int): Projection dimensionality.\n",
    "\n",
    "    Returns:\n",
    "      Tensor multiplied to the scale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, init_values, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.init_values = init_values\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tensorflow.Variable(\n",
    "            self.init_values * tensorflow.ones((self.projection_dim,))\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * self.gamma\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"init_values\": self.init_values,\n",
    "                \"projection_dim\": self.projection_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "num_classes = 19\n",
    "model = keras.models.load_model(r\"F:\\Pre-Trained_Models\\Pre-Trained_Models\\conv_whoi_224_model.h5\",custom_objects={ \"LayerScale\": LayerScale })\n",
    "\n",
    "x = model.layers[-4].output  # Access the last 4th layer from the end\n",
    "x = GlobalAveragePooling2D(keepdims=False)(x)  # Explicitly specify keepdims\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "new_model = keras.models.Model(inputs=model.input, outputs=output) \n",
    "new_model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "epochs = 50\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "history = new_model.fit(X_train, YT, epochs=epochs, batch_size=8, validation_data=(X_val, VT), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d9482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a65728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = new_model.predict(X_test)\n",
    "\n",
    "# Convert one-hot encoded predictions back to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, predicted_labels)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(Y_test, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2399a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af4cf43d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Duplicate registrations for type trackable_dict_wrapper",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, GlobalAveragePooling2D\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      7\u001b[0m     hub\u001b[38;5;241m.\u001b[39mKerasLayer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.kaggle.com/models/spsayakpaul/convnext/frameworks/TensorFlow2/variations/base-1k-224/versions/1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m ])\n\u001b[0;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39moutput  \u001b[38;5;66;03m# Access the last 4th layer from the end\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_hub\\__init__.py:117\u001b[0m\n\u001b[0;32m    111\u001b[0m _ensure_keras_2_importable()\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Symbols exposed via tensorflow_hub.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLayer\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resolve\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_hub\\keras_layer.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_inspect\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_structures\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtracking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_structures\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\trackable\\data_structures.py:1101\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_function\u001b[39m(x):\n\u001b[0;32m   1098\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (def_function\u001b[38;5;241m.\u001b[39mFunction, defun\u001b[38;5;241m.\u001b[39mConcreteFunction))\n\u001b[1;32m-> 1101\u001b[0m \u001b[43mrevived_types\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_revived_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrackable_dict_wrapper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_DictWrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mrevived_types\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVersionedTypeRegistration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Standard dependencies are enough to reconstruct the trackable\u001b[39;49;00m\n\u001b[0;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# items in dictionaries, so we don't need to save any extra information.\u001b[39;49;00m\n\u001b[0;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobject_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_DictWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_producer_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_consumer_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43msetter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetitem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_list_item\u001b[39m(list_object, index_string, value):\n\u001b[0;32m   1115\u001b[0m   item_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index_string)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\saved_model\\revived_types.py:138\u001b[0m, in \u001b[0;36mregister_revived_type\u001b[1;34m(identifier, predicate, versions)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# TODO(kathywu): Remove the \"optimizer\" special case here after the Keras\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# repo optimizer registration has been submitted.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m identifier \u001b[38;5;129;01min\u001b[39;00m _REVIVED_TYPE_REGISTRY \u001b[38;5;129;01mand\u001b[39;00m identifier \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 138\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate registrations for type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(identifier))\n\u001b[0;32m    141\u001b[0m _REVIVED_TYPE_REGISTRY[identifier] \u001b[38;5;241m=\u001b[39m (predicate, versions)\n\u001b[0;32m    142\u001b[0m _TYPE_IDENTIFIERS\u001b[38;5;241m.\u001b[39mappend(identifier)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Duplicate registrations for type trackable_dict_wrapper"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\"https://www.kaggle.com/models/spsayakpaul/convnext/frameworks/TensorFlow2/variations/base-1k-224/versions/1\")\n",
    "])\n",
    "\n",
    "x = model.layers[-4].output  # Access the last 4th layer from the end\n",
    "x = GlobalAveragePooling2D(keepdims=False)(x)  # Explicitly specify keepdims\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "new_model = tf.keras.models.Model(inputs=model.input, outputs=output) \n",
    "new_model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "epochs = 50\n",
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(X_train,YT, epochs=epochs,batch_size=8, validation_data=(X_val,VT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e464a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = new_model.predict(X_test)\n",
    "\n",
    "# Convert one-hot encoded predictions back to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, predicted_labels)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(Y_test, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7138ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5d206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7123599f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b067f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676c5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013f8666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
